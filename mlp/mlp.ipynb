{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch import utils\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_kmnist_train(path, is_train='train'):\n",
    "    labels_path = os.path.join(path,f'{is_train}-labels-idx1-ubyte')\n",
    "    images_path = os.path.join(path,f'{is_train}-images-idx3-ubyte')\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 1, 28, 28)\n",
    "    return images.astype(np.float32), labels, is_train\n",
    "\n",
    "class kmnistDataset(utils.data.Dataset):\n",
    "  def __init__(self, file_path, is_train):\n",
    "    self.features, self.labels, process_type = read_kmnist_train(file_path, is_train)\n",
    "    print(\"read \"+str(len(self.features))+f' {process_type} examples')\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.features[idx], self.labels[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.features)\n",
    "\n",
    "whole_set = kmnistDataset(\"./data\", is_train=\"train\")\n",
    "length = len(whole_set)\n",
    "train_size,validate_size=int(0.8*length),int(0.2*length)\n",
    "batch_size, lr, num_epochs = 64, 0.1, 10\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "\n",
    "train_dataset,val_dataset=torch.utils.data.random_split(whole_set,[train_size,validate_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4\n",
    "net_4 = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 10)\n",
    "                    )\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net_4.apply(init_weights)\n",
    "net = net_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 6\n",
    "net_6 = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 10)\n",
    "                    )\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net_6.apply(init_weights)\n",
    "net = net_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 20\n",
    "def sub_net_6():\n",
    "  return nn.Sequential(#6\n",
    "                    nn.Linear(1024, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 4096),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4096, 8192),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8192, 4096),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4096, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 1024),\n",
    "                    nn.ReLU())\n",
    "\n",
    "net_20 = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    sub_net_6(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, 16),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 10)\n",
    "                    )\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net_20.apply(init_weights)\n",
    "net = net_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 25, but this 25 layers model is prone to memory overflow\n",
    "def sub_net_5():\n",
    "  return nn.Sequential(#6\n",
    "                    nn.Linear(1024, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 4096),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4096, 8192),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8192, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 1024),\n",
    "                    nn.ReLU())\n",
    "\n",
    "net_25 = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    sub_net_6(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    sub_net_5(),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, 16),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 10)\n",
    "                    )\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net_25.apply(init_weights)\n",
    "net = net_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "if not os.path.exists(\"./weight/\"):\n",
    "  os.makedirs(\"./weight/\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "  if isinstance(net, torch.nn.Module):\n",
    "    net.eval() \n",
    "    if not device:\n",
    "      device = next(iter(net.parameters())).device\n",
    "  metric = Accumulator(2) \n",
    "  for X,y in data_iter:\n",
    "    if isinstance(X, list): \n",
    "      X = [x.to(device) for x in X]\n",
    "    else:\n",
    "      X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    metric.add(accuracy(net(X),y), y.numel()) \n",
    "  return metric[0]/metric[1] \n",
    "\n",
    "def get_dataloader_workers():\n",
    "    return 4\n",
    "\n",
    "class Accumulator:\n",
    "  def __init__(self, n): \n",
    "    self.data = [0.0] * n \n",
    "  def add(self, *args):\n",
    "    self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "  def reset(self):\n",
    "    self.data = [0.0] * len(self.data)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def train_cls_process(net, train_iter, test_iter, num_epochs, lr, device, save_path=None):\n",
    "    result = \"\"\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    net.apply(init_weights) \n",
    "    print(\"training on\", device)\n",
    "    net.to(device) \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr) \n",
    "    loss = nn.CrossEntropyLoss() \n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(3) \n",
    "        net.train()\n",
    "        for i, (X,y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l*X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            train_l = metric[0]/metric[2]\n",
    "            train_acc = metric[1]/metric[2]\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(net.state_dict(), f\"./weights/mlp.pth\")\n",
    "\n",
    "        print(f'epoch {epoch+1}, train loss {train_l:f}, val_acc {test_acc:f}')\n",
    "        result += f\"{train_l}, {test_acc}\\n\"\n",
    "    print(f'train loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "            f'val acc {test_acc:.3f}, best acc {best_acc:.3f}' )\n",
    "    with open(f\"./test/mlp__pro_params.txt\", \"w\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\") \n",
    "train_cls_process(net, train_loader, val_loader, num_epochs, lr=0.001,  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    test_imgs, test_labels, is_train = read_kmnist_train(\n",
    "        \"./data\",is_train=\"t10k\"\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pre_clas = []\n",
    "    total_loss = 0\n",
    "    model = net.to(device)\n",
    "    model_weight_path = f\"./weights/mlp.pth\"\n",
    "    model.load_state_dict(torch.load(model_weight_path,map_location=device))\n",
    "    model.eval()\n",
    "    for i in range(len(test_imgs)):\n",
    "        test_img, test_label = torch.Tensor(test_imgs[i]), torch.from_numpy(np.array(test_labels[i]))\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        test_img = torch.unsqueeze(test_img, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = torch.squeeze(model(test_img.to(device))).cpu()\n",
    "            predict = torch.softmax(output, dim=0)\n",
    "            pre_cla = torch.argmax(predict)# \n",
    "            loss = loss_function(output, test_label.long())\n",
    "            total_loss += loss\n",
    "\n",
    "        pre_clas.append(str(pre_cla.numpy()))\n",
    "\n",
    "    avg_loss = total_loss/int(len(test_imgs))\n",
    "    loss_ = str(avg_loss.numpy())\n",
    "\n",
    "    result = \",\".join(pre_clas)\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"./test/\"):\n",
    "        os.makedirs(\"./test/\")\n",
    "    \n",
    "    with open(f\"./test/mlp_result.txt\", \"w\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    with open(f\"./test/mlp_avgloss.txt\", \"w\") as f:\n",
    "        f.write(loss_)\n",
    "    print(f\"the test based on mlp is done, and its average loss value is {loss_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = np.loadtxt('./test/best_mlp_pred_result.txt', dtype=int, delimiter=',')\n",
    "y_true = np.loadtxt('./test_label.txt', dtype=int, delimiter=',')\n",
    "mlp = confusion_matrix(y_true, y_pred_mlp)\n",
    "print(\"Confusion matrix of MLP:\")\n",
    "print(mlp)\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred_mlp))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = mlp)\n",
    "disp.plot(\n",
    "    include_values=True,            # 混淆矩阵每个单元格上显示具体数值\n",
    "    cmap=\"viridis\",                 # 使用的sklearn中的默认值\n",
    "    ax=None,                        # 同上\n",
    "    xticks_rotation=\"horizontal\",   # 同上\n",
    "    values_format=\"d\"               # 显示的数值格式\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a956f358629cb9341796b8882b8e6b546aa77209204abab8c69d48eff56608a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
